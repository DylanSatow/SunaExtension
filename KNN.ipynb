{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class JoinAwareMIEstimator:\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Initialize MI estimator with k nearest neighbors\n",
    "        \n",
    "        Args:\n",
    "            k: Number of nearest neighbors to use\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self._valid_keys = None  # Cache for valid join keys\n",
    "        self._x_values = None  # Cache for valid X values\n",
    "        self._y_values = None  # Cache for valid Y values\n",
    "\n",
    "    def _compute_distances(self, X, Y, join_key):\n",
    "        \"\"\"\n",
    "        Compute k-nearest neighbor distances without materializing full join\n",
    "        \n",
    "        Args:\n",
    "            X: Dictionary mapping join_key -> x_value \n",
    "            Y: Dictionary mapping join_key -> y_value\n",
    "            join_key: Array of join keys that exist in both X and Y\n",
    "        \n",
    "        Returns:\n",
    "            epsilon: Array of k-th nearest neighbor distances\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for key in join_key:\n",
    "            if key not in X or key not in Y:\n",
    "                continue\n",
    "\n",
    "            x, y = X[key], Y[key]\n",
    "            # Find k-th nearest neighbor distance using L-infinity norm\n",
    "            x_dists = [abs(x - X[k2]) for k2 in join_key if k2 in X]\n",
    "            y_dists = [abs(y - Y[k2]) for k2 in join_key if k2 in Y]\n",
    "            x_dists.sort()\n",
    "            y_dists.sort()\n",
    "\n",
    "            if len(x_dists) < self.k or len(y_dists) < self.k:\n",
    "                continue\n",
    "\n",
    "            dist = max(x_dists[self.k-1], y_dists[self.k-1])\n",
    "            distances.append(dist)\n",
    "\n",
    "        return np.array(distances)\n",
    "\n",
    "    def _count_neighbors(self, X, Y, join_key, epsilon):\n",
    "        \"\"\"\n",
    "        Count points within epsilon radius for each dimension\n",
    "        \n",
    "        Args:\n",
    "            X, Y: Dictionaries mapping join_key -> value\n",
    "            join_key: Array of valid join keys\n",
    "            epsilon: Array of distances for each point\n",
    "            \n",
    "        Returns:\n",
    "            nx, ny: Arrays containing number of neighbors in each dimension\n",
    "        \"\"\"\n",
    "        nx, ny = [], []\n",
    "\n",
    "        for i, key in enumerate(join_key):\n",
    "            if key not in X or key not in Y:\n",
    "                continue\n",
    "\n",
    "            x, y = X[key], Y[key]\n",
    "            eps = epsilon[i]\n",
    "\n",
    "            # Count neighbors within epsilon using L-infinity norm\n",
    "            x_neighbors = sum(1 for k2 in join_key if k2 in X and abs(X[k2] - x) <= eps)\n",
    "            y_neighbors = sum(1 for k2 in join_key if k2 in Y and abs(Y[k2] - y) <= eps)\n",
    "\n",
    "            nx.append(x_neighbors)\n",
    "            ny.append(y_neighbors)\n",
    "\n",
    "        return np.array(nx), np.array(ny)\n",
    "\n",
    "    def estimate(self, X, Y, join_key=None):\n",
    "        \"\"\"\n",
    "        Estimate mutual information between X and Y using k-nearest neighbors\n",
    "        \n",
    "        Args:\n",
    "            X: Dictionary mapping join_key -> x_value\n",
    "            Y: Dictionary mapping join_key -> y_value \n",
    "            join_key: Optional array of join keys present in both X and Y. If None, will compute intersection of X and Y keys.\n",
    "            \n",
    "        Returns:\n",
    "            mi: Estimated mutual information value\n",
    "        \"\"\"\n",
    "        # Compute valid keys if not provided\n",
    "        if join_key is None:\n",
    "            join_key = list(set(X.keys()) & set(Y.keys()))\n",
    "\n",
    "        # Cache valid keys and values\n",
    "        self._valid_keys = set(join_key)  # Convert to set for O(1) lookup\n",
    "        self._x_values = np.array([X[k] for k in join_key])\n",
    "        self._y_values = np.array([Y[k] for k in join_key])\n",
    "        # Get k-nearest distances\n",
    "        epsilon = self._compute_distances(X, Y, join_key)\n",
    "        if len(epsilon) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Count neighbors within epsilon radius\n",
    "        nx, ny = self._count_neighbors(X, Y, join_key, epsilon)\n",
    "\n",
    "        # Compute MI estimate using digamma function\n",
    "        n = len(epsilon)\n",
    "        mi = digamma(self.k) - np.mean(digamma(nx) + digamma(ny)) + digamma(n)\n",
    "\n",
    "        return max(0, mi)  # MI should be non-negative\n",
    "\n",
    "\n",
    "def generate_test_data(n_samples=1000, correlation=0.5, noise=0.1):\n",
    "    \"\"\"Generate correlated normal distributions with known MI\"\"\"\n",
    "    # Generate correlated normal data\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, correlation], [correlation, 1]]\n",
    "    x, y = np.random.multivariate_normal(mean, cov, n_samples).T\n",
    "\n",
    "    # Add some noise\n",
    "    x += np.random.normal(0, noise, n_samples)\n",
    "    y += np.random.normal(0, noise, n_samples)\n",
    "\n",
    "    # Convert to dictionary format with keys\n",
    "    X = {f'key{i}': val for i, val in enumerate(x)}\n",
    "    Y = {f'key{i}': val for i, val in enumerate(y)}\n",
    "\n",
    "    # Theoretical MI for bivariate normal is -0.5 * log(1 - correlation^2)\n",
    "    theoretical_mi = -0.5 * np.log(1 - correlation**2)\n",
    "\n",
    "    return X, Y, theoretical_mi\n",
    "\n",
    "\n",
    "def test_mi_estimators(n_samples=1000, correlations=[0.0, 0.3, 0.6, 0.9], k=3):\n",
    "    print(\"\\nTesting MI Estimators\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Correlation':^12} | {'Theoretical':^12} | {\n",
    "          'Sklearn':^12} | {'JoinAware':^12} | {'Time Ratio':^12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for correlation in correlations:\n",
    "        # Generate test data\n",
    "        X_dict, Y_dict, theoretical_mi = generate_test_data(\n",
    "            n_samples, correlation)\n",
    "        join_keys = list(X_dict.keys())\n",
    "\n",
    "        # Convert to array format for sklearn\n",
    "        X_arr = np.array([X_dict[k] for k in join_keys]).reshape(-1, 1)\n",
    "        y_arr = np.array([Y_dict[k] for k in join_keys])\n",
    "\n",
    "        # Time sklearn implementation\n",
    "        t0 = time.time()\n",
    "        sklearn_mi = mutual_info_regression(X_arr, y_arr, n_neighbors=k)[0]\n",
    "        sklearn_time = time.time() - t0\n",
    "\n",
    "        # Time our implementation\n",
    "        t0 = time.time()\n",
    "        estimator = JoinAwareMIEstimator(k=k)\n",
    "        our_mi = estimator.estimate(X_dict, Y_dict, join_keys)\n",
    "        our_time = time.time() - t0\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{correlation:^12.3f} | {theoretical_mi:^12.3f} | {\n",
    "              sklearn_mi:^12.3f} | {our_mi:^12.3f} | {our_time/sklearn_time:^12.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Test with different sample sizes\n",
    "    for n_samples in [100, 1000, 10000]:\n",
    "        print(f\"\\nTesting with {n_samples} samples:\")\n",
    "        test_mi_estimators(n_samples=n_samples)\n",
    "\n",
    "    # Test with different k values\n",
    "    print(\"\\nTesting with different k values (1000 samples):\")\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        print(f\"\\nk = {k}:\")\n",
    "        test_mi_estimators(n_samples=1000, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Let it compute valid keys\n",
    "estimator = JoinAwareMIEstimator(k=3)\n",
    "mi = estimator.estimate(X_dict, Y_dict)\n",
    "\n",
    "# Method 2: Provide pre-computed join keys\n",
    "valid_keys = list(set(X_dict.keys()) & set(Y_dict.keys()))\n",
    "mi = estimator.estimate(X_dict, Y_dict, join_key=valid_keys)\n",
    "\n",
    "# Method 3: Provide specific subset of keys\n",
    "important_keys = ['id1', 'id2']  # Only compute MI for these keys\n",
    "mi = estimator.estimate(X_dict, Y_dict, join_key=important_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
